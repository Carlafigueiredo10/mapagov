"""
Base Helena - Contrato Abstrato para Produtos Conversacionais

Define o contrato que todos os produtos Helena devem seguir.
Garante consistÃªncia na interface e facilita extensibilidade.
"""
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional
import logging
import time

logger = logging.getLogger(__name__)


class BaseHelena(ABC):
    """
    Classe abstrata base para todos os produtos Helena.

    Responsabilidades:
    - Define interface padrÃ£o (processar, inicializar_estado)
    - Gerencia versionamento do agente
    - Fornece mÃ©todos auxiliares comuns

    Todas as Helenas devem:
    1. Ser stateless (sem self.estado)
    2. Receber estado via parÃ¢metro
    3. Retornar novo estado no resultado
    """

    VERSION = "1.0.0"  # Versionamento semÃ¢ntico (MAJOR.MINOR.PATCH)
    PRODUTO_NOME = "Helena Base"  # Sobrescrever nas subclasses

    def __init__(self):
        """InicializaÃ§Ã£o bÃ¡sica - nÃ£o deve conter estado mutÃ¡vel"""
        logger.info(f"Inicializando {self.PRODUTO_NOME} v{self.VERSION}")

    @abstractmethod
    def processar(self, mensagem: str, session_data: dict) -> dict:
        """
        Processa uma mensagem do usuÃ¡rio.

        Args:
            mensagem: Texto enviado pelo usuÃ¡rio
            session_data: Estado atual da sessÃ£o (JSON do DB/Redis)
                Estrutura esperada: {
                    'etapa_atual': int,
                    'dados_coletados': dict,
                    'historico': list,
                    ...
                }

        Returns:
            dict com estrutura:
            {
                'resposta': str,  # Resposta para o usuÃ¡rio
                'novo_estado': dict,  # Estado atualizado (serÃ¡ salvo)
                'progresso': Optional[str],  # Ex: "2/5 etapas"
                'sugerir_contexto': Optional[str],  # Ex: 'fluxograma'
                'metadados': Optional[dict]  # Dados extras (tokens usados, etc)
            }

        Raises:
            ValueError: Se mensagem vazia ou session_data invÃ¡lido
        """
        pass

    @abstractmethod
    def inicializar_estado(self) -> dict:
        """
        Retorna estado inicial limpo para este produto.

        Returns:
            dict: Estado inicial (serÃ¡ usado na primeira interaÃ§Ã£o)

        Exemplo:
            {'etapa_atual': 0, 'dados_coletados': {}, 'historico': []}
        """
        pass

    def get_version(self) -> str:
        """Retorna versÃ£o do agente"""
        return self.VERSION

    def get_nome(self) -> str:
        """Retorna nome do produto"""
        return self.PRODUTO_NOME

    def validar_mensagem(self, mensagem: str) -> None:
        """
        Valida mensagem de entrada.

        Args:
            mensagem: Texto a validar

        Raises:
            ValueError: Se mensagem invÃ¡lida
        """
        if not mensagem or not isinstance(mensagem, str):
            raise ValueError("Mensagem deve ser uma string nÃ£o-vazia")

        if len(mensagem.strip()) == 0:
            raise ValueError("Mensagem nÃ£o pode ser apenas espaÃ§os")

        if len(mensagem) > 10000:  # Limite de seguranÃ§a
            raise ValueError("Mensagem muito longa (mÃ¡x 10.000 caracteres)")

    def validar_session_data(self, session_data: dict) -> None:
        """
        Valida estrutura de session_data.

        Args:
            session_data: Estado da sessÃ£o

        Raises:
            ValueError: Se session_data invÃ¡lido
        """
        if not isinstance(session_data, dict):
            raise ValueError("session_data deve ser um dicionÃ¡rio")

    def criar_resposta(
        self,
        resposta: str,
        novo_estado: dict,
        progresso: Optional[str] = None,
        sugerir_contexto: Optional[str] = None,
        metadados: Optional[dict] = None,
        tipo_interface: Optional[str] = None,
        dados_interface: Optional[dict] = None,
        formulario_pop: Optional[dict] = None,  # âœ… FASE 2: Novo nome
        dados_extraidos: Optional[dict] = None  # âœ… FIX: Compatibilidade com frontend OLD
    ) -> dict:
        """
        Helper para criar resposta padronizada.

        Args:
            resposta: Texto de resposta
            novo_estado: Estado atualizado
            progresso: String de progresso (ex: "3/5")
            sugerir_contexto: Contexto sugerido para mudanÃ§a
            metadados: Dados extras
            tipo_interface: Tipo de interface dinÃ¢mica (ex: 'areas', 'dropdown_macro')
            dados_interface: Dados para renderizar a interface
            formulario_pop: Dados do formulÃ¡rio POP (FASE 2 - novo nome)
            dados_extraidos: Dados extraÃ­dos (compatibilidade com frontend OLD)

        Returns:
            dict: Resposta formatada
        """
        # ğŸ¯ DISTINÃ‡ÃƒO SEMÃ‚NTICA: Modo Interface vs Modo Textual
        # - Modo Interface: resposta pode ser None (interface substitui texto)
        # - Modo Textual: resposta Ã© obrigatÃ³ria (chat conversacional puro)

        if tipo_interface and dados_interface:
            # âœ… MODO INTERFACE: Interface dinÃ¢mica (botÃµes, cards, listas, etc)
            # resposta pode ser None se a interface mostra tudo (pureza arquitetural)
            resultado = {
                'resposta': resposta,  # Pode ser None quando interface substitui texto
                'interface': tipo_interface,
                'tipo_interface': tipo_interface,  # Compatibilidade dupla
                'dados': dados_interface,
                'dados_interface': dados_interface,  # Compatibilidade dupla
                'novo_estado': novo_estado,
            }
        else:
            # âœ… MODO TEXTUAL: Chat conversacional puro
            # Apenas texto, sem interface
            resultado = {
                'resposta': resposta or "",  # Garantir string vazia em vez de None
                'interface': None,
                'tipo_interface': None,
                'dados': None,
                'dados_interface': None,
                'novo_estado': novo_estado,
            }

        # Campos comuns a ambos os modos
        if progresso:
            resultado['progresso'] = progresso

        if sugerir_contexto:
            resultado['sugerir_contexto'] = sugerir_contexto

        if metadados:
            resultado['metadados'] = metadados

        if formulario_pop:
            resultado['formulario_pop'] = formulario_pop

        if dados_extraidos:
            resultado['dados_extraidos'] = dados_extraidos

        return resultado

    def detectar_intencao_mudanca_contexto(self, mensagem: str) -> Optional[str]:
        """
        Detecta se usuÃ¡rio quer mudar de contexto.

        Args:
            mensagem: Texto do usuÃ¡rio

        Returns:
            str: Nome do contexto sugerido ou None

        Exemplo:
            "quero fazer um fluxograma" -> "fluxograma"
            "como mapear riscos?" -> "riscos"
        """
        msg_lower = mensagem.lower()

        # Mapeamento de palavras-chave para contextos
        contextos = {
            'fluxograma': ['fluxograma', 'diagrama', 'flowchart'],
            'riscos': ['risco', 'riscos', 'anÃ¡lise de risco'],
            'pop': ['pop', 'procedimento', 'operacional padrÃ£o'],
            'mapeamento': ['mapear', 'mapeamento', 'processo'],
            'conformidade': ['conformidade', 'compliance', 'regulamento'],
        }

        for contexto, palavras_chave in contextos.items():
            if any(palavra in msg_lower for palavra in palavras_chave):
                return contexto

        return None

    def log_event(
        self,
        evento: str,
        dados: Optional[Dict[str, Any]] = None,
        nivel: str = "info"
    ) -> None:
        """
        Registra evento estruturado com contexto.

        Args:
            evento: Nome do evento (ex: "estado_mudou", "erro_validacao")
            dados: Dados adicionais do evento
            nivel: NÃ­vel de log ("debug", "info", "warning", "error", "critical")

        Exemplo:
            self.log_event("estado_mudou", {
                "de": "nome_usuario",
                "para": "area_decipex",
                "timestamp": time.time()
            })
        """
        # Criar contexto estruturado
        contexto = {
            "produto": self.PRODUTO_NOME,
            "versao": self.VERSION,
            "evento": evento,
            "timestamp": time.time(),
        }

        if dados:
            contexto.update(dados)

        # Log estruturado
        log_msg = f"[{self.PRODUTO_NOME}] {evento}"

        if nivel == "debug":
            logger.debug(log_msg, extra=contexto)
        elif nivel == "info":
            logger.info(log_msg, extra=contexto)
        elif nivel == "warning":
            logger.warning(log_msg, extra=contexto)
        elif nivel == "error":
            logger.error(log_msg, extra=contexto)
        elif nivel == "critical":
            logger.critical(log_msg, extra=contexto)
        else:
            logger.info(log_msg, extra=contexto)

    def chamar_modelo(
        self,
        prompt: str,
        provider: str = "vertex",
        model: Optional[str] = None,
        temperatura: float = 0.7,
        max_tokens: int = 1000
    ) -> Dict[str, Any]:
        """
        Hook genÃ©rico para chamar modelos de IA (Vertex AI, OpenAI, etc).

        Args:
            prompt: Prompt a ser enviado ao modelo
            provider: Provider do modelo ("vertex", "openai", "anthropic")
            model: Nome especÃ­fico do modelo (opcional, usa padrÃ£o do provider)
            temperatura: Temperatura de geraÃ§Ã£o (0.0-1.0)
            max_tokens: NÃºmero mÃ¡ximo de tokens

        Returns:
            dict: {
                'texto': str,  # Resposta do modelo
                'tokens_usados': int,
                'modelo': str,
                'provider': str
            }

        Raises:
            NotImplementedError: Se provider nÃ£o implementado
            Exception: Se erro ao chamar API

        Exemplo:
            resposta = self.chamar_modelo(
                prompt="Sugira 3 normas para este processo",
                provider="vertex",
                temperatura=0.3
            )
            print(resposta['texto'])
        """
        # Log evento
        self.log_event("chamar_modelo", {
            "provider": provider,
            "model": model,
            "temperatura": temperatura
        }, nivel="debug")

        # ImplementaÃ§Ãµes por provider
        if provider == "vertex":
            return self._chamar_vertex(prompt, model, temperatura, max_tokens)
        elif provider == "openai":
            return self._chamar_openai(prompt, model, temperatura, max_tokens)
        elif provider == "anthropic":
            return self._chamar_anthropic(prompt, model, temperatura, max_tokens)
        else:
            raise NotImplementedError(f"Provider '{provider}' nÃ£o implementado")

    def _chamar_vertex(
        self,
        prompt: str,
        model: Optional[str],
        temperatura: float,
        max_tokens: int
    ) -> Dict[str, Any]:
        """ImplementaÃ§Ã£o para Vertex AI"""
        try:
            from google.cloud import aiplatform
            from vertexai.preview.generative_models import GenerativeModel

            if model is None:
                model = "gemini-pro"

            gen_model = GenerativeModel(model)
            response = gen_model.generate_content(
                prompt,
                generation_config={
                    "temperature": temperatura,
                    "max_output_tokens": max_tokens
                }
            )

            return {
                'texto': response.text,
                'tokens_usados': response.usage_metadata.total_token_count if hasattr(response, 'usage_metadata') else 0,
                'modelo': model,
                'provider': 'vertex'
            }

        except ImportError:
            logger.warning("google-cloud-aiplatform nÃ£o instalado")
            raise NotImplementedError("Vertex AI nÃ£o disponÃ­vel (biblioteca nÃ£o instalada)")
        except Exception as e:
            logger.error(f"Erro ao chamar Vertex AI: {e}")
            raise

    def _chamar_openai(
        self,
        prompt: str,
        model: Optional[str],
        temperatura: float,
        max_tokens: int
    ) -> Dict[str, Any]:
        """ImplementaÃ§Ã£o para OpenAI"""
        try:
            import openai

            if model is None:
                model = "gpt-3.5-turbo"

            response = openai.ChatCompletion.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=temperatura,
                max_tokens=max_tokens
            )

            return {
                'texto': response.choices[0].message.content,
                'tokens_usados': response.usage.total_tokens,
                'modelo': model,
                'provider': 'openai'
            }

        except ImportError:
            logger.warning("openai nÃ£o instalado")
            raise NotImplementedError("OpenAI nÃ£o disponÃ­vel (biblioteca nÃ£o instalada)")
        except Exception as e:
            logger.error(f"Erro ao chamar OpenAI: {e}")
            raise

    def _chamar_anthropic(
        self,
        prompt: str,
        model: Optional[str],
        temperatura: float,
        max_tokens: int
    ) -> Dict[str, Any]:
        """ImplementaÃ§Ã£o para Anthropic Claude"""
        try:
            import anthropic

            if model is None:
                model = "claude-3-sonnet-20240229"

            client = anthropic.Anthropic()
            response = client.messages.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=temperatura,
                max_tokens=max_tokens
            )

            return {
                'texto': response.content[0].text,
                'tokens_usados': response.usage.input_tokens + response.usage.output_tokens,
                'modelo': model,
                'provider': 'anthropic'
            }

        except ImportError:
            logger.warning("anthropic nÃ£o instalado")
            raise NotImplementedError("Anthropic nÃ£o disponÃ­vel (biblioteca nÃ£o instalada)")
        except Exception as e:
            logger.error(f"Erro ao chamar Anthropic: {e}")
            raise

    def __str__(self) -> str:
        return f"{self.PRODUTO_NOME} v{self.VERSION}"

    def __repr__(self) -> str:
        return f"<{self.__class__.__name__}(version='{self.VERSION}')>"
