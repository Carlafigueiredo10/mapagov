# âš¡ OTIMIZAÃ‡ÃƒO MEMÃ“RIA: Lazy loading de LangChain
# Imports movidos para dentro da funÃ§Ã£o

def criar_helena_mapeamento():
    """
    Cria Helena como parceira de preenchimento do POP.
    Ela responde de forma empÃ¡tica, clara e motivadora.
    MantÃ©m memÃ³ria da conversa para dar continuidade real.
    Ela informa com tom leve de humor que muitas coisas o usuÃ¡rio vai aprender ao longo do processo de mapeamento.
    Helena cuidadosamente conduz o usuÃ¡rio a voltar pro fluxo do mapeamento.
    Helena sÃ³ muda de assunto quando o usuÃ¡rio confirmar que a dÃºvida foi resolvida.
    """

    # âš¡ Lazy imports
    from langchain_openai import ChatOpenAI
    from langchain.prompts import ChatPromptTemplate
    from langchain.chains import LLMChain
    from langchain.memory import ConversationBufferMemory

    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.6)

    prompt = ChatPromptTemplate.from_messages([
        ("system",
         "VocÃª Ã© Helena, assistente especialista em mapeamento de processos e POPs. "
         "Seu papel Ã© ser parceira do usuÃ¡rio: clara, paciente, empÃ¡tica e bem-humorada. "
         "O usuÃ¡rio estÃ¡ preenchendo os campos de um POP e vocÃª deve ajudÃ¡-lo com explicaÃ§Ãµes simples, exemplos prÃ¡ticos e incentivo. "
         "Mantenha foco no campo atual atÃ© o usuÃ¡rio confirmar que a dÃºvida foi resolvida. "
         "Nunca repita a mesma explicaÃ§Ã£o: traga novos exemplos, metÃ¡foras ou perguntas de apoio. "
         "SÃ³ avance para outro campo se o usuÃ¡rio pedir explicitamente. "
         "Use humor leve e reconhecimento do esforÃ§o quando fizer sentido. "
         "No fim de cada resposta, confirme com o usuÃ¡rio: 'Essa etapa ficou clara?' ou 'Podemos seguir?'. "
        ),
        ("human", "{input}")
    ])

    # ðŸ”¹ MemÃ³ria para manter histÃ³rico e contexto
    memory = ConversationBufferMemory(return_messages=True)

    # ðŸ”¹ Chain combina LLM + prompt + memÃ³ria
    chain = LLMChain(
        llm=llm,
        prompt=prompt,
        memory=memory
    )

    def responder(mensagem: str):
        resposta = chain.run(input=mensagem)
        return resposta

    return responder

# âš¡ LAZY LOADING: InstÃ¢ncia criada sob demanda
_helena_mapeamento_instance = None

def helena_mapeamento(mensagem: str):
    """FunÃ§Ã£o wrapper para lazy loading da instÃ¢ncia Helena Mapeamento"""
    global _helena_mapeamento_instance
    if _helena_mapeamento_instance is None:
        _helena_mapeamento_instance = criar_helena_mapeamento()
    return _helena_mapeamento_instance(mensagem)


# ============================================================================
# CLASSE BASEHELENA PARA INTEGRAÃ‡ÃƒO COM HELENA CORE
# ============================================================================

from processos.domain.base import BaseHelena
from typing import Dict, Any


class HelenaMapeamento(BaseHelena):
    """
    Helena Mapeamento - Produto conversacional para tirar dÃºvidas sobre processos.

    Usa LLM (GPT-4o-mini) para responder livremente perguntas sobre:
    - O que Ã© POP
    - O que Ã© CAP
    - Como funciona o mapeamento
    - Qualquer outra dÃºvida do usuÃ¡rio

    MantÃ©m conversaÃ§Ã£o atÃ© usuÃ¡rio indicar que terminou.
    """

    VERSION = "1.0.0"
    PRODUTO_NOME = "Helena Mapeamento"

    def __init__(self):
        super().__init__()
        self._chain_instance = None  # Lazy loading do LangChain

    def inicializar_estado(self) -> dict:
        """
        Retorna estado inicial para Helena Mapeamento.

        Returns:
            dict: Estado inicial com histÃ³rico de mensagens vazio
        """
        return {
            'historico_mensagens': [],
            'contexto': None,  # Contexto da dÃºvida (ex: "explicacao_pop")
            'nome_usuario': None,
            'contador_mensagens': 0,
            'finalizou': False
        }

    def processar(self, mensagem: str, session_data: dict) -> dict:
        """
        Processa mensagem delegando para LLM.

        Args:
            mensagem: Pergunta/dÃºvida do usuÃ¡rio
            session_data: Estado atual com histÃ³rico

        Returns:
            dict: Resposta com novo estado e flag de finalizaÃ§Ã£o
        """
        self.validar_mensagem(mensagem)
        self.validar_session_data(session_data)

        # Lazy loading da chain LangChain
        if self._chain_instance is None:
            self._chain_instance = criar_helena_mapeamento()

        # Processar mensagem com LLM
        resposta_llm = self._chain_instance(mensagem)

        # Atualizar histÃ³rico
        session_data['historico_mensagens'].append({
            'role': 'user',
            'content': mensagem
        })
        session_data['historico_mensagens'].append({
            'role': 'assistant',
            'content': resposta_llm
        })
        session_data['contador_mensagens'] += 1

        # Detectar se usuÃ¡rio quer finalizar
        msg_lower = mensagem.lower().strip()
        finalizou = any(palavra in msg_lower for palavra in [
            'entendi', 'obrigad', 'valeu', 'ok agora', 'ficou claro',
            'vamos seguir', 'podemos continuar', 'seguir', 'continuar'
        ])

        # ApÃ³s 5+ mensagens, perguntar se pode finalizar
        if session_data['contador_mensagens'] >= 5 and not session_data['finalizou']:
            finalizou = True

        session_data['finalizou'] = finalizou

        return {
            'resposta': resposta_llm,
            'novo_estado': session_data,
            'finalizou_duvidas': finalizou,  # Flag para Helena POP
            'progresso': f"{session_data['contador_mensagens']} mensagens",
            'metadados': {
                'contador_mensagens': session_data['contador_mensagens'],
                'contexto': session_data.get('contexto')
            }
        }